{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/StopWords\"\n",
    "\n",
    "# Initialize an empty list to store all words\n",
    "stop_words = []\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # Open the file and read its contents with the correct encoding\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            # Read the entire content of the file\n",
    "            file_content = file.read()\n",
    "            # Split the content into words\n",
    "            words = file_content.split()\n",
    "            # Add the words to the list of all words\n",
    "            stop_words.extend(words)\n",
    "\n",
    "print(\"Total number of words:\", len(stop_words))\n",
    "\n",
    "# Saving the file\n",
    "file_path = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/stop_words.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, \"w\") as file:\n",
    "    # Write each word to the file on separate lines\n",
    "    for word in stop_words:\n",
    "        file.write(word + \"\\n\")\n",
    "\n",
    "# Creating a dictionary of Positive and Negative words. We add only those words in the dictionary if they are not found in the Stop Words Lists. \n",
    "        \n",
    "# Read stop words list from the file\n",
    "def read_stopwords(stopwords_file):\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as file:\n",
    "        stopwords = [line.strip() for line in file]\n",
    "    return stopwords\n",
    "\n",
    "# Define the file paths\n",
    "positive_file = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/MasterDictionary/positive-words.txt\"\n",
    "negative_file = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/MasterDictionary/negative-words.txt\"\n",
    "stopwords_file = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/stop_words.txt\"\n",
    "\n",
    "# Read stop words\n",
    "stopwords = read_stopwords(stopwords_file)\n",
    "\n",
    "# Initialize dictionary for positive and negative words\n",
    "word_dict = {'positive': [], 'negative': []}\n",
    "\n",
    "# Function to process words from a file\n",
    "def process_words(file_path, word_list):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            # Check if the word is not in the stop words list\n",
    "            if word not in stopwords:\n",
    "                word_list.append(word)\n",
    "\n",
    "\n",
    "# Process positive words\n",
    "process_words(positive_file, word_dict['positive'])\n",
    "\n",
    "# Process negative words\n",
    "process_words(negative_file, word_dict['negative'])\n",
    "\n",
    "print(\"Dictionary of positive and negative words without stop words:\")\n",
    "print(word_dict)\n",
    "\n",
    "# Write positive words to positive_dict.txt\n",
    "with open(\"positive_dict.txt\", \"w\") as positive_file:\n",
    "    for word in word_dict['positive']:\n",
    "        positive_file.write(word + \"\\n\")\n",
    "\n",
    "# Write negative words to negative_dict.txt\n",
    "with open(\"negative_dict.txt\", \"w\") as negative_file:\n",
    "    for word in word_dict['negative']:\n",
    "        negative_file.write(word + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/suvankarmaity/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/suvankarmaity/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suvankarmaity/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to count words in a file\n",
    "def count_words(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        word_count = len(content.split())\n",
    "    return word_count\n",
    "\n",
    "# Function to load words from file\n",
    "def load_words_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return set(word.strip().lower() for word in f)\n",
    "\n",
    "# Function to count words in a file based on a given set of words\n",
    "def count_words_in_file(file_path, words):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read().lower()\n",
    "        return sum(1 if word in words else 0 for word in content.split())\n",
    "\n",
    "# Function to calculate polarity score and subjectivity score\n",
    "def calculate_polarity_and_subjectivity_scores(folder_path, positive_file, negative_file):\n",
    "    positive_words = load_words_from_file(positive_file)\n",
    "    negative_words = load_words_from_file(negative_file)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            positive_score = count_words_in_file(file_path, positive_words)\n",
    "            negative_score = count_words_in_file(file_path, negative_words)\n",
    "            total_words = count_words(file_path)\n",
    "            polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "            subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "            scores.append({\n",
    "                \"File Name\": file_name,\n",
    "                \"Positive Score\": positive_score,\n",
    "                \"Negative Score\": negative_score,\n",
    "                \"Polarity Score\": polarity_score,\n",
    "                \"Subjectivity Score\": subjectivity_score\n",
    "            })\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Function to count the number of complex words in a list of words\n",
    "def count_complex_words(words):\n",
    "    cmu_dict = cmudict.dict()\n",
    "    complex_word_count = 0\n",
    "    for word in words:\n",
    "        syllables = syllable_count(word, cmu_dict)\n",
    "        if syllables > 2:\n",
    "            complex_word_count += 1\n",
    "    return complex_word_count\n",
    "\n",
    "# Function to count the number of syllables in a word using the CMU Pronouncing Dictionary\n",
    "def syllable_count(word, cmu_dict):\n",
    "    if word.lower() in cmu_dict:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in cmu_dict[word.lower()]])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate the average sentence length\n",
    "def calculate_average_sentence_length(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        sentences = sent_tokenize(content)\n",
    "        words = word_tokenize(content)\n",
    "        return len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "\n",
    "# Function to calculate the percentage of complex words\n",
    "def calculate_percentage_complex_words(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        words = word_tokenize(content)\n",
    "        total_words = len(words)\n",
    "        complex_word_count = count_complex_words(words)\n",
    "        if total_words == 0:\n",
    "            return 0  # Prevent division by zero\n",
    "        else:\n",
    "            return (complex_word_count / total_words) * 100\n",
    "\n",
    "# Function to calculate the Fog Index for a file\n",
    "def calculate_fog_index(file_path):\n",
    "    average_sentence_length = calculate_average_sentence_length(file_path)\n",
    "    percentage_complex_words = calculate_percentage_complex_words(file_path)\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "    return fog_index\n",
    "\n",
    "# Function to analyze readability metrics for each file\n",
    "def analyze_readability(folder_path):\n",
    "    readability_metrics = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            average_sentence_length = calculate_average_sentence_length(file_path)\n",
    "            percentage_complex_words = calculate_percentage_complex_words(file_path)\n",
    "            fog_index = calculate_fog_index(file_path)\n",
    "            readability_metrics.append({\n",
    "                \"File Name\": file_name,\n",
    "                \"Average Sentence Length\": average_sentence_length,\n",
    "                \"Percentage of Complex Words\": percentage_complex_words,\n",
    "                \"Fog Index\": fog_index\n",
    "            })\n",
    "\n",
    "    return readability_metrics\n",
    "\n",
    "# Function to merge scores and readability metrics and store in CSV\n",
    "def merge_and_store_data(folder_path, positive_file, negative_file, output_file):\n",
    "    polarity_subjectivity_scores = calculate_polarity_and_subjectivity_scores(folder_path, positive_file, negative_file)\n",
    "    readability_metrics = analyze_readability(folder_path)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    for score_data, readability_data in zip(polarity_subjectivity_scores, readability_metrics):\n",
    "        merged_data.append({**score_data, **readability_data})\n",
    "\n",
    "    # Write data to CSV\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = merged_data[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for data in merged_data:\n",
    "            writer.writerow(data)\n",
    "\n",
    "# Define paths\n",
    "folder_path = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/removed_stopwords_data\"\n",
    "positive_file = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/positive_dict.txt\"\n",
    "negative_file = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/negative_dict.txt\"\n",
    "output_file = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/metrics1_data.csv\"\n",
    "\n",
    "# Merge data and store in CSV\n",
    "merge_and_store_data(folder_path, positive_file, negative_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/suvankarmaity/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/suvankarmaity/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suvankarmaity/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key metrics data has been saved to: /Users/suvankarmaity/Downloads/Meghla Internship Project/key_metrics_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to count the number of words and sentences in a file\n",
    "def count_words_and_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        sentences = sent_tokenize(content)\n",
    "        words = word_tokenize(content)\n",
    "        return len(words), len(sentences)\n",
    "\n",
    "# Function to count the number of complex words in a list of words\n",
    "def count_complex_words(words):\n",
    "    cmu_dict = cmudict.dict()\n",
    "    complex_word_count = 0\n",
    "    for word in words:\n",
    "        syllables = syllable_count(word, cmu_dict)\n",
    "        if syllables > 2:\n",
    "            complex_word_count += 1\n",
    "    return complex_word_count\n",
    "\n",
    "# Function to count the number of syllables in a word using the CMU Pronouncing Dictionary\n",
    "def syllable_count(word, cmu_dict):\n",
    "    if word.lower() in cmu_dict:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in cmu_dict[word.lower()]])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to count the total cleaned words in a file\n",
    "def count_cleaned_words(file_path):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_word_count = 0\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        words = word_tokenize(content)\n",
    "        \n",
    "        # Remove punctuations and stop words\n",
    "        cleaned_words = [word.lower() for word in words if word.lower() not in stop_words and word.lower() not in punctuation]\n",
    "        \n",
    "        cleaned_word_count = len(cleaned_words)\n",
    "\n",
    "    return cleaned_word_count\n",
    "\n",
    "# Function to count personal pronouns in text\n",
    "def count_personal_pronouns(text):\n",
    "    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "    # Regular expression pattern to match personal pronouns\n",
    "    pattern = r'\\b(?:' + '|'.join(personal_pronouns) + r')\\b'\n",
    "    # Find all matches of personal pronouns in the text\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "    return len(matches)\n",
    "\n",
    "# Function to calculate average word length in text\n",
    "def calculate_average_word_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "    if total_words == 0:\n",
    "        return 0  # Prevent division by zero\n",
    "    else:\n",
    "        return total_characters / total_words\n",
    "\n",
    "# Function to collect key metrics for each file\n",
    "def collect_key_metrics_for_files(folder_path):\n",
    "    data = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                words, sentences = count_words_and_sentences(file_path)\n",
    "                cmplx_words = count_complex_words(word_tokenize(content))\n",
    "                cleaned_words = count_cleaned_words(file_path)\n",
    "                pronouns = count_personal_pronouns(content)\n",
    "                avg_word_length = calculate_average_word_length(content)\n",
    "                data.append([file_name, words / sentences, cmplx_words, cleaned_words, pronouns, avg_word_length])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Define the path to the folder containing the files\n",
    "folder_path = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/removed_stopwords_data\"\n",
    "\n",
    "# Collect key metrics for each file in the folder\n",
    "metrics_data = collect_key_metrics_for_files(folder_path)\n",
    "\n",
    "# Write the key metrics data into a CSV file\n",
    "csv_file_path = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/metrics2_data.csv\"\n",
    "with open(csv_file_path, \"w\", encoding=\"utf-8\") as csv_file:\n",
    "    # Write header\n",
    "    csv_file.write(\"File Name,Average Number of Words Per Sentence,Complex Word Count,Cleaned Word Count,Personal Pronoun Count,Average Word Length\\n\")\n",
    "    # Write data\n",
    "    for row in metrics_data:\n",
    "        csv_file.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(\"Key metrics data has been saved to:\", csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data has been saved to: Output Data Structure.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the two CSV files\n",
    "file1 = pd.read_csv(\"/Users/suvankarmaity/Downloads/Meghla Internship Project/metrics1_data.csv\")\n",
    "file2 = pd.read_csv(\"/Users/suvankarmaity/Downloads/Meghla Internship Project/metrics2_data.csv\")\n",
    "\n",
    "# Merge the two files based on the 'File Name' column\n",
    "merged_data = pd.merge(file1, file2, on='File Name', how='inner')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(\"Output Data Structure.csv\", index=False)\n",
    "\n",
    "print(\"Merged data has been saved to: Output Data Structure.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/suvankarmaity/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/suvankarmaity/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suvankarmaity/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key metrics data has been saved to: /Users/suvankarmaity/Downloads/Meghla Internship Project/metrics2_data.csv\n",
      "Merged data has been saved to: Output Data Structure.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to count words in a file\n",
    "def count_words(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        word_count = len(content.split())\n",
    "    return word_count\n",
    "\n",
    "# Function to load words from file\n",
    "def load_words_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return set(word.strip().lower() for word in f)\n",
    "\n",
    "# Function to count words in a file based on a given set of words\n",
    "def count_words_in_file(file_path, words):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read().lower()\n",
    "        return sum(1 if word in words else 0 for word in content.split())\n",
    "\n",
    "# Function to calculate polarity score and subjectivity score\n",
    "def calculate_polarity_and_subjectivity_scores(folder_path, positive_file, negative_file):\n",
    "    positive_words = load_words_from_file(positive_file)\n",
    "    negative_words = load_words_from_file(negative_file)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            positive_score = count_words_in_file(file_path, positive_words)\n",
    "            negative_score = count_words_in_file(file_path, negative_words)\n",
    "            total_words = count_words(file_path)\n",
    "            polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "            subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "            scores.append({\n",
    "                \"File Name\": file_name,\n",
    "                \"Positive Score\": positive_score,\n",
    "                \"Negative Score\": negative_score,\n",
    "                \"Polarity Score\": polarity_score,\n",
    "                \"Subjectivity Score\": subjectivity_score\n",
    "            })\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Function to count the number of complex words in a list of words\n",
    "def count_complex_words(words):\n",
    "    cmu_dict = cmudict.dict()\n",
    "    complex_word_count = 0\n",
    "    for word in words:\n",
    "        syllables = syllable_count(word, cmu_dict)\n",
    "        if syllables > 2:\n",
    "            complex_word_count += 1\n",
    "    return complex_word_count\n",
    "\n",
    "# Function to count the number of syllables in a word using the CMU Pronouncing Dictionary\n",
    "def syllable_count(word, cmu_dict):\n",
    "    if word.lower() in cmu_dict:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in cmu_dict[word.lower()]])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate the average sentence length\n",
    "def calculate_average_sentence_length(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        sentences = sent_tokenize(content)\n",
    "        words = word_tokenize(content)\n",
    "        return len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "\n",
    "# Function to calculate the percentage of complex words\n",
    "def calculate_percentage_complex_words(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        words = word_tokenize(content)\n",
    "        total_words = len(words)\n",
    "        complex_word_count = count_complex_words(words)\n",
    "        if total_words == 0:\n",
    "            return 0  # Prevent division by zero\n",
    "        else:\n",
    "            return (complex_word_count / total_words) * 100\n",
    "\n",
    "# Function to calculate the Fog Index for a file\n",
    "def calculate_fog_index(file_path):\n",
    "    average_sentence_length = calculate_average_sentence_length(file_path)\n",
    "    percentage_complex_words = calculate_percentage_complex_words(file_path)\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "    return fog_index\n",
    "\n",
    "# Function to analyze readability metrics for each file\n",
    "def analyze_readability(folder_path):\n",
    "    readability_metrics = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            average_sentence_length = calculate_average_sentence_length(file_path)\n",
    "            percentage_complex_words = calculate_percentage_complex_words(file_path)\n",
    "            fog_index = calculate_fog_index(file_path)\n",
    "            readability_metrics.append({\n",
    "                \"File Name\": file_name,\n",
    "                \"Average Sentence Length\": average_sentence_length,\n",
    "                \"Percentage of Complex Words\": percentage_complex_words,\n",
    "                \"Fog Index\": fog_index\n",
    "            })\n",
    "\n",
    "    return readability_metrics\n",
    "\n",
    "# Function to merge scores and readability metrics and store in CSV\n",
    "def merge_and_store_data(folder_path, positive_file, negative_file, output_file):\n",
    "    polarity_subjectivity_scores = calculate_polarity_and_subjectivity_scores(folder_path, positive_file, negative_file)\n",
    "    readability_metrics = analyze_readability(folder_path)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    for score_data, readability_data in zip(polarity_subjectivity_scores, readability_metrics):\n",
    "        merged_data.append({**score_data, **readability_data})\n",
    "\n",
    "    # Write data to CSV\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = merged_data[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for data in merged_data:\n",
    "            writer.writerow(data)\n",
    "\n",
    "# Define paths\n",
    "folder_path = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/removed_stopwords_data\"\n",
    "positive_file = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/positive_dict.txt\"\n",
    "negative_file = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/negative_dict.txt\"\n",
    "output_file = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/metrics1_data.csv\"\n",
    "\n",
    "# Merge data and store in CSV\n",
    "merge_and_store_data(folder_path, positive_file, negative_file, output_file)\n",
    "\n",
    "\n",
    "# Function to count the number of words and sentences in a file\n",
    "def count_words_and_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        sentences = sent_tokenize(content)\n",
    "        words = word_tokenize(content)\n",
    "        return len(words), len(sentences)\n",
    "\n",
    "# Function to count the number of complex words in a list of words\n",
    "def count_complex_words(words):\n",
    "    cmu_dict = cmudict.dict()\n",
    "    complex_word_count = 0\n",
    "    for word in words:\n",
    "        syllables = syllable_count(word, cmu_dict)\n",
    "        if syllables > 2:\n",
    "            complex_word_count += 1\n",
    "    return complex_word_count\n",
    "\n",
    "# Function to count the number of syllables in a word using the CMU Pronouncing Dictionary\n",
    "def syllable_count(word, cmu_dict):\n",
    "    if word.lower() in cmu_dict:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in cmu_dict[word.lower()]])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to count the total cleaned words in a file\n",
    "def count_cleaned_words(file_path):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_word_count = 0\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        words = word_tokenize(content)\n",
    "        \n",
    "        # Remove punctuations and stop words\n",
    "        cleaned_words = [word.lower() for word in words if word.lower() not in stop_words and word.lower() not in punctuation]\n",
    "        \n",
    "        cleaned_word_count = len(cleaned_words)\n",
    "\n",
    "    return cleaned_word_count\n",
    "\n",
    "# Function to count personal pronouns in text\n",
    "def count_personal_pronouns(text):\n",
    "    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "    # Regular expression pattern to match personal pronouns\n",
    "    pattern = r'\\b(?:' + '|'.join(personal_pronouns) + r')\\b'\n",
    "    # Find all matches of personal pronouns in the text\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "    return len(matches)\n",
    "\n",
    "# Function to calculate average word length in text\n",
    "def calculate_average_word_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "    if total_words == 0:\n",
    "        return 0  # Prevent division by zero\n",
    "    else:\n",
    "        return total_characters / total_words\n",
    "\n",
    "# Function to collect key metrics for each file\n",
    "def collect_key_metrics_for_files(folder_path):\n",
    "    data = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                words, sentences = count_words_and_sentences(file_path)\n",
    "                cmplx_words = count_complex_words(word_tokenize(content))\n",
    "                cleaned_words = count_cleaned_words(file_path)\n",
    "                pronouns = count_personal_pronouns(content)\n",
    "                avg_word_length = calculate_average_word_length(content)\n",
    "                data.append([file_name, words / sentences, cmplx_words, cleaned_words, pronouns, avg_word_length])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Define the path to the folder containing the files\n",
    "folder_path = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/removed_stopwords_data\"\n",
    "\n",
    "# Collect key metrics for each file in the folder\n",
    "metrics_data = collect_key_metrics_for_files(folder_path)\n",
    "\n",
    "# Write the key metrics data into a CSV file\n",
    "csv_file_path = \"/Users/suvankarmaity/Downloads/Meghla Internship Project/metrics2_data.csv\"\n",
    "with open(csv_file_path, \"w\", encoding=\"utf-8\") as csv_file:\n",
    "    # Write header\n",
    "    csv_file.write(\"File Name,Average Number of Words Per Sentence,Complex Word Count,Cleaned Word Count,Personal Pronoun Count,Average Word Length\\n\")\n",
    "    # Write data\n",
    "    for row in metrics_data:\n",
    "        csv_file.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "print(\"Key metrics data has been saved to:\", csv_file_path)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the two CSV files\n",
    "file1 = pd.read_csv(\"/Users/suvankarmaity/Downloads/Meghla Internship Project/metrics1_data.csv\")\n",
    "file2 = pd.read_csv(\"/Users/suvankarmaity/Downloads/Meghla Internship Project/metrics2_data.csv\")\n",
    "\n",
    "# Merge the two files based on the 'File Name' column\n",
    "merged_data = pd.merge(file1, file2, on='File Name', how='inner')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(\"Output Data Structure.csv\", index=False)\n",
    "\n",
    "print(\"Merged data has been saved to: Output Data Structure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
